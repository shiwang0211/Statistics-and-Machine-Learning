{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why usually discretize continuous variables?\n",
    "- Robust to extreme values (e.g., age = 300)\n",
    "- Introduce non-linearity (e.g., age < 10, age = 10-15, age > 30)\n",
    "- Easier for feature interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBDT (Gradient Boosting Decision Tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"http://xijun-album.oss-cn-hangzhou.aliyuncs.com/Ensembling/p5.png\" width = \"500\">\n",
    "\n",
    "---\n",
    "\n",
    "<img src = \"http://xijun-album.oss-cn-hangzhou.aliyuncs.com/Ensembling/p6.png\" width = \"500\">\n",
    "\n",
    "---\n",
    "\n",
    "<img src = \"http://explained.ai/gradient-boosting/images/latex-CB3574D4B05979222377D8458B38FCF4.svg\" width = \"500\">\n",
    "\n",
    "- Each node split, find best **feature** and **split point** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Special cases:**\n",
    "\n",
    "1. Square error --> Boosting Decesion Tree to fit residuals\n",
    "\n",
    "<img src = \"http://explained.ai/gradient-boosting/images/latex-321A7951E78381FB73D2A6874916134D.svg\" width = \"500\">\n",
    "\n",
    "\n",
    "2. Absolute error\n",
    "\n",
    "<img src = \"http://explained.ai/gradient-boosting/images/latex-99749CB3C601B0DD9BEE5A9E91049D4B.svg\" width = \"500\">\n",
    "\n",
    "3. Exponential error\n",
    "    - $l(y,f)=exp(-yf(x))$\n",
    "    - Recovers Adaboost Algorithm\n",
    "    - sensitive to noise data\n",
    "\n",
    "\n",
    "4. Logistic loss\n",
    "    - $l(y, p) = -[y ln(p) + (1-y)ln(1-p)]$ ; where y = 0,1\n",
    "    - $l(y, f) = - y ln[\\frac{1}{1+exp(-f(x))}] - (1-y)ln[\\frac{1}{1+exp(f(x))}] = ln[1+exp(-yf(x))]$\n",
    "    - $r_{m-1} = \\frac{\\partial l}{\\partial f} = \\frac{y}{1+exp(yf(x))}$\n",
    "    \n",
    "<img src=\"https://slideplayer.com/6982498/24/images/53/Boosting+and+Logistic+Regression.jpg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBT\n",
    "    \n",
    "- Added regularization for trees (Number of leaves + L2 norm of leave weights) for better generalization\n",
    "<img src = \"http://xijun-album.oss-cn-hangzhou.aliyuncs.com/Ensembling/p7.png\" width = \"300\">\n",
    "\n",
    "- Taylor Expansion Approximation of Loss\n",
    "    - In GBDT, we have first-order derivative (negative gradient)\n",
    "    - Generally we have $f(x + \\Delta x) = f(x) + f'(x)\\Delta x + \\frac{1}{2}f''(x) (\\Delta x)^2 + ...$\n",
    "    - In this case: \n",
    "<img src = \"http://xijun-album.oss-cn-hangzhou.aliyuncs.com/Ensembling/p8.png\" width = \"400\">\n",
    "\n",
    "<img src = \"http://xijun-album.oss-cn-hangzhou.aliyuncs.com/Ensembling/p9.png\" width = \"500\">\n",
    "   \n",
    "- The goal of tree each iteration is to find a decision tree $f_t(X)$ so as to minimize objective $\\sum [g_if_t(x_i) + \\frac {1}{2} h_i f_t^2(x_i)] + \\Omega(f_t)$\n",
    "\n",
    "\n",
    "- Node split (Gain + Complexity Cost)\n",
    "    - In GBDT, squared error is minimized\n",
    "    - Directly bind the split criteria to the minimization goal defined in previous step\n",
    "<img src = \"http://xijun-album.oss-cn-hangzhou.aliyuncs.com/Ensembling/p10.png\" width = \"500\">\n",
    "\n",
    "\n",
    "- Other improvements\n",
    "    - Random subset of features of each node just like random forest to reduce variance\n",
    "    - Parallel feature finding at each node to improve computational speed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare bagging with boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bagging (RF)\n",
    "    - Focus: reduce variance\n",
    "    - Reduce variance by building independent trees and aggregating\n",
    "    - Reduce bias by using deeper trees\n",
    "    \n",
    "    \n",
    "- Boosting (GBDT)\n",
    "    - Focus: reduce bias\n",
    "    - Reduce variance by using shallow/simple trees\n",
    "    - Reduce bias by sequentially fitting error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- http://explained.ai/gradient-boosting/index.html\n",
    "- https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "230px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
