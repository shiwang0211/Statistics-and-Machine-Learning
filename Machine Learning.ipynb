{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "- Want to: learn $P(c|x)$\n",
    "    - Discriminative models (like LR)\n",
    "    - Generative models: based on $P(x|c)$\n",
    "    \n",
    "    \n",
    "- For generative model:\n",
    "    - $P(c|x) = \\frac{P(x,c)}{P(x)} = \\frac{P(c)P(x|c)}{P(x)}$\n",
    "    - Prior: $P(c)$\n",
    "    - Likelihood: $P(x|c)$\n",
    "    \n",
    "    \n",
    "- How to describe $P(x|c)$\n",
    "    - $P(x|c) = P(x|\\theta_c)$\n",
    "    - For continuous variables: $P(x|c) = N(\\mu_c, \\sigma^2_c)$ (*depends on right assumption*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Motivation: Calculate $P(x|c)$, which is a joint distribution, cannot be estimated from limited samples (curse of dimension).\n",
    "\n",
    "- Attribute conditional independence assumption: $P(x|c) = \\prod_i P(x_i|c)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: flip coins**\n",
    "- Represent data by some parameters: $P(D|\\theta) = \\prod_i P(Y_i|\\theta) = \\theta^{N_+} (1-\\theta)^{N_-}$\n",
    "\n",
    "- Set prior for $\\theta$: $P(\\theta)$\n",
    "\n",
    "- Calculate posterior for $\\theta$: $P(\\theta|D) = \\frac{P(D|\\theta)P(\\theta)}{P(D)}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM (Expectation-Maximization) algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $Z$ is latent variable. Cannot be obsewrved.\n",
    "- $LL(\\Theta|X,Z) = ln P(X,Z|\\Theta)$\n",
    "- $LL(\\Theta|X) = ln P(X|\\Theta) = ln \\sum_z P(X,Z|\\Theta)$\n",
    "- For GMM clustering: $\\mathbf Z = (k_1, k_2, ..., k_i, ...)$, the class label for each data point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Expectation: $Q(\\boldsymbol\\theta|\\boldsymbol\\theta^{(t)}) = \\operatorname{E}_{\\mathbf{Z}|\\mathbf{X},\\boldsymbol\\theta^{(t)}}\\left[ \\log L (\\boldsymbol\\theta;\\mathbf{X},\\mathbf{Z})  \\right] $\n",
    "\n",
    "\n",
    "- Maximization: $\\boldsymbol\\theta^{(t+1)} = \\underset{\\boldsymbol\\theta}{\\operatorname{arg\\,max}} \\ Q(\\boldsymbol\\theta|\\boldsymbol\\theta^{(t)})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised - Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validity Index\n",
    "- External index (compare with reference model)\n",
    "- Internal index\n",
    "    - Intra-cluster average/max distance\n",
    "    - Inter-cluster min distance, centroid distance, etc.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GMM (Gaussian Mixture Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Known class labels:**\n",
    "- $\\mu_k=\\sum_i x_{i,k} / N_k$\n",
    "- $\\sigma^2_k = \\frac{\\sum_i (x_{i,k} - \\mu_k)^2}{N_k}$\n",
    "\n",
    "**Unknown class labels**:\n",
    "- Objective is to maximize likelihood\n",
    "    - $P(\\mathbf X) = P(\\mathbf X |\\mu,\\sigma^2, \\alpha) = \\sum_k \\alpha_k P(\\mathbf X|\\mu_k,\\sigma^2_k) $\n",
    "- Approach: *Soft Labels*\n",
    "\n",
    "\n",
    "- Initialization to assign each sample $i$ to class $k$.\n",
    "- $\\mu_k=\\sum_i x_{i,k} / N_k$\n",
    "- $\\sigma^2_k = \\frac{\\sum_i (x_{i,k} - \\mu_k)^2}{N_k}$\n",
    "- $\\alpha_k = \\frac{N_k}{N}$\n",
    "\n",
    "\n",
    "***Until Convergence***\n",
    "- Expectation (E) step: \n",
    "    - Calculate the probability of sample $i$ belongs to each class from 1 to $K$\n",
    "    - $p(k|x_i) = \\frac{p(k)p(x_i|k)}{p(x_i)} = \\frac{\\alpha_{k}N(x_i|\\mu_{k}, \\sigma^2_{k})}{\\sum_{k}\\alpha_{k}N(x_i|\\mu_{k}, \\sigma^2_{k})}$\n",
    "\n",
    "\n",
    "- Maximization (M) step: \n",
    "    - Re-estimate paramaters\n",
    "    - $\\mu_k = \\frac{\\sum_ip(k|x_i)x_i}{\\sum_ip(k|x_i)}$\n",
    "    - $\\sigma^2_k = \\frac{\\sum_ip(k|x_i)(x_i-\\mu_k)^2}{\\sum_ip(k|x_i)}$\n",
    "    - $\\alpha_k = \\frac{\\sum_ip(k|x_i)}{N}$\n",
    "    \n",
    "    \n",
    "- Detailed proof for GMM clsutering: https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-training\n",
    "- Train $f$ from $(X_ll, Y_l)$\n",
    "- Predict on $x ∈ X_u$\n",
    "- Add $(x, f(x))$ to labeled data\n",
    "- Repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Assumption: labeled and unlabelled comes from the same mixed distribution\n",
    "- Compare with GMM-based clustering: labels of unlabelled data can be viewed as latent variable $Z$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One way of formulating:  $p(X_l, Y_l, X_u|\\theta) = \\sum_{Y_u}p(X_l, Y_l, X_u, Y_u|\\theta)$\n",
    "\n",
    "- The combined log-likelihood: $${\\underset {\\Theta }{\\operatorname {argmax} }}\\left(\\log p(\\{x_{i},y_{i}\\}_{i=1}^{l}|\\theta )+\\lambda \\log p(\\{x_{i}\\}_{i=l+1}^{l+u}|\\theta )\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where: \n",
    "- Labelled: $\\log p(\\{x_{i},y_{i}=k_i\\}_{i=1}^{l}|\\theta ) = \\sum_i ln \\ p(x_i,k_i|\\theta) = \\sum_i ln\\ \\alpha_{k_i}p(x_i|\\mu_{k_i}, \\sigma^2_{k_i}) $\n",
    "\n",
    "\n",
    "- Unlabelled: $\\log p(\\{x_{i}\\}_{i=l+1}^{l+u}|\\theta ) = \\ln \\sum_i p(x_i|\\theta) = \\sum_i \\sum_k \\ln \\alpha_k  p(x_i|\\mu_k, \\sigma^2_k) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be solved by EM algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi-supervised Support Vector Machines (S3VM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- http://pages.cs.wisc.edu/~jerryzhu/pub/sslicml07.pdf\n",
    "- Main idea: Add loss/penalty term for unlabelled data:\n",
    "$$ Min \\sum_i [1 - y_i f(x_i)]_+ \\lambda_1 ||w||^2) + \\lambda_2 \\sum_i (1-|f(x_i)|)_+$$\n",
    "<img src=\"./fig/s3vm.png\" width=\"200\">\n",
    "<img src=\"./fig/hinge_unlabelled.png\" width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph-based methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PU Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All positive samples + unlabelled samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use Information Gain to split nodes\n",
    "    - Information entropy: $E_0 = -\\sum_k p_k log_2p_k$, where $k$ is class index\n",
    "    - Information Gain $Gain= E_0 - \\sum_{node} N_{node}\\% \\times E_{node}$ \n",
    "    - Maximize information gain -> Increase in \"purity\"\n",
    "    - ***Example: ID3***\n",
    "\n",
    "\n",
    "- Use Information Gain Ratio to split nodes    \n",
    "    - Drawback of information gain: best result will be using \"ID\" column (i.e., perfect split)\n",
    "    - Fix: Information Gain *Ratio* $Gain\\ Ratio = \\frac{Gain}{IV_{feature}}$: to normalized based on number of distinct values\n",
    "    - ***Example: C4.5***: mixed use of information gain and information gain ratio\n",
    "    \n",
    "    \n",
    "- Use Gini index to split node\n",
    "    - Gini = $1 - \\sum_k p^2_k$\n",
    "    - Gini index $= \\sum_{node} N_{node} \\times Gini_{node}$ \n",
    "    - Minimize gini index -> Increase in \"purity\"\n",
    "    - ***Example: CART***\n",
    "\n",
    "\n",
    "- Continous variable and Missing values\n",
    "\n",
    "\n",
    "- Decision Boundary and Multivariate Decision Tree\n",
    "    - When splitting node, select attribute *set* instead of single attribute (i.e., left: $w^Tx>0$ , right: $w^Tx<0$)\n",
    "    \n",
    "    \n",
    "- Node splitting for *Regression Tree*\n",
    "    - Find feature $j$ and splitting point $s$ so that:\n",
    "     $$Min_{j,s} [min_{c_{left}}\\sum_{left}(y_i - c_{left})^2 + min_{c_{right}}\\sum_{left}(y_i - c_{right})^2]$$\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to formulate the problem?\n",
    "- Most Intuitive formulation\n",
    "    - Maximize Geometric Margin: $\\gamma $\n",
    "    - Constraint: $\\gamma_i = \\frac{y_i (w^Tx_i + b)}{||w||} \\geq \\gamma $\n",
    "    \n",
    "    \n",
    "- Define functional margin:\n",
    "    - $\\gamma_i = \\frac{\\hat \\gamma_i}{||w||}$, where ${\\hat \\gamma_i}$ is function margin: $y_i f(x_i$)\n",
    "    - Maximize Functional Margin:$Max \\frac{\\hat \\gamma}{||w||}$\n",
    "    - Constraint: $\\hat \\gamma_i = {y_i (w^Tx_i + b)} \\geq \\hat \\gamma $\n",
    "\n",
    "\n",
    "- Take a step further:\n",
    "    - Scaling $w$ and $b$ by $\\hat \\gamma$ will not affect decision boundary\n",
    "    - Maximize: $\\frac{1}{||w||}$\n",
    "    - Constraint: ${y_i (w^Tx_i + b)} \\geq 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See Notes below for how to re-formulate the problem:\n",
    "- 1) Original problem: $Min ||w||^2$\n",
    "- 2) Unconstraint problem $Min_w\\ Max_{\\alpha, \\beta}\\ L(\\alpha, \\beta, w)$\n",
    "- 3) Dual problem: $Max_{\\alpha, \\beta}\\ Min_w\\ L(\\alpha, \\beta, w)$ with constraints.\n",
    "- 4) With SVM satisfying *Slater Condition*, solve dual problem equivalent to orginal problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function:\n",
    "- Min $\\lambda ||w||^2 + \\sum_i[1-y_i(w^Tx_i + b)]_+$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel function\n",
    "- Popular kernels (linear, Polynomial, RBF)\n",
    "    - Polynomial: $k(x_1, x_2) = (x_1, x_2 +c)^d$\n",
    "    - RBF: $k(x_1, x_2) = exp(- \\frac{||x_1 - x_2||^2}{2\\sigma^2})$\n",
    "- How to select kernel\n",
    "    - CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why usually discretize continuous variables?\n",
    "- Robust to extreme values (e.g., age = 300)\n",
    "- Introduce non-linearity (e.g., age < 10, age = 10-15, age > 30)\n",
    "- Easier for feature interaction\n",
    "\n",
    "## Algorithm \n",
    "\n",
    "- Prediction function: $p = \\frac{1}{1+exp(-f(x))}$, where $f(x) = \\theta^Tx$\n",
    "- Loss function: \n",
    "    - $L = \\sum_i ln[p(y_i|x_i;\\mathbf w)]$; where y = 0,1\n",
    "    - $l(y, p) = -[y ln(p) + (1-y)ln(1-p)]$ ; where y = 0,1\n",
    "\n",
    "\n",
    "- Exponential Loss: \n",
    "    - $l(y, f) = - y ln[\\frac{1}{1+exp(-f(x))}] - (1-y)ln[\\frac{1}{1+exp(f(x))}]$; where y = 0,1\n",
    "    - $l(y, f) = ln[1+exp(-yf(x))]$, where y = -1, +1\n",
    "\n",
    "\n",
    "- Gradient wrt $f(x)$\n",
    "    - $r_{m-1} = \\frac{\\partial l}{\\partial f} = (y-p)$; where y = 0,1\n",
    "    - $r_{m-1} = \\frac{\\partial l}{\\partial f} = \\frac{y}{1+exp(yf(x))}$, where y = -1, +1\n",
    "\n",
    "\n",
    "- Gradient wrt $\\theta$\n",
    "    - $\\frac{\\partial L}{\\partial \\theta_j} = \\frac{\\partial L}{\\partial f} \\frac{\\partial f}{\\partial \\theta_j} = \\sum_i(p_i-y_i)x_i^j$ ; where y = 0,1\n",
    "    - $\\theta_j := \\theta_j - \\alpha \\frac{\\partial L}{\\partial \\theta_j}$\n",
    "\n",
    "\n",
    "- How to prove convex optimization: \n",
    "    - $L = \\sum_i [-y_i \\mathbf w x_i + ln (1+e^{\\mathbf w x_i})]$ ; where y = 0,1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBDT (Gradient Boosting Decision Tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"http://xijun-album.oss-cn-hangzhou.aliyuncs.com/Ensembling/p5.png\" width = \"500\">\n",
    "\n",
    "---\n",
    "\n",
    "<img src = \"http://xijun-album.oss-cn-hangzhou.aliyuncs.com/Ensembling/p6.png\" width = \"500\">\n",
    "\n",
    "---\n",
    "\n",
    "<img src = \"http://explained.ai/gradient-boosting/images/latex-CB3574D4B05979222377D8458B38FCF4.svg\" width = \"500\">\n",
    "\n",
    "\n",
    "**Summary**:\n",
    "- $r_{i,m}$ is the negative gradient direction for function $f$.\n",
    "- A regression tree is used to fit the negative gradient direction. (e.g., residual vector in squared error loss)\n",
    "- Each node split, find best **feature** and **split point**\n",
    "- Another optimization problem is solved to find estimated value for each region (i.e., linear search for step size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Special cases:**\n",
    "\n",
    "1. Square error --> Boosting Decesion Tree to fit residuals\n",
    "\n",
    "<img src = \"http://explained.ai/gradient-boosting/images/latex-321A7951E78381FB73D2A6874916134D.svg\" width = \"500\">\n",
    "\n",
    "\n",
    "2. Absolute error\n",
    "\n",
    "<img src = \"http://explained.ai/gradient-boosting/images/latex-99749CB3C601B0DD9BEE5A9E91049D4B.svg\" width = \"500\">\n",
    "\n",
    "3. Exponential error\n",
    "    - $l(y,f)=exp(-yf(x))$ , where y = -1, +1\n",
    "    - Recovers Adaboost Algorithm\n",
    "    - sensitive to noise data \n",
    "<img src=\"./fig/ExponentialLoss.png\" width=\"600\">\n",
    "\n",
    "---\n",
    "\n",
    "**Adaboost**\n",
    "\n",
    "- Classifer at iteration $m-1$: $f_{m-1}(x) = \\alpha_{1}\\phi_{1}(x) + \\alpha_{2}\\phi_{2}(x) + ... + \\alpha_{m-1}\\phi_{m-1}(x)  $\n",
    "\n",
    "- Classifer at iteration $m$: $f_m(x) = f_{m-1}(x) + \\alpha_m \\phi_m(x)$\n",
    "\n",
    "\n",
    "- Minimize exponential Loss: $L(y, f) = exp(-yf(x)) =\\sum_i exp(-y_i f_m(x_i)) = \\sum_i [exp(-y_i f_{m-1}(x_i)][exp(-\\alpha_m y_i \\phi_m(x_i))] = \\sum_i w_{m,i}\\ exp(-\\alpha_m y_i \\phi_m(x_i))$\n",
    "- It can be shown that $w_{m,i}= exp(- \\alpha_{m-1}y_i \\phi_{m-1}(x_i)) \\times ... \\times  exp(- \\alpha_{1}y_i \\phi_{1}(x_i))$\n",
    "    - $w_{m+1, i} = w_{m,i} \\times exp(-\\alpha_t)$ when correct\n",
    "    - $w_{m+1, i} = w_{m,i} \\times exp(\\alpha_t)$ when wrong\n",
    "\n",
    "\n",
    "- Optimal solution: ($\\alpha^*_m, \\phi^*_m(x)) = argmin\\ L $\n",
    "\n",
    "\n",
    "- Obviously, $\\phi^*_m(x)$ is not affected by the value of $\\alpha_m >0$, \n",
    "    - $\\phi^*_m(x)$ = $argmin \\sum_i w_{m,i} I(y_i \\neq \\phi_m(x_i))$ (i.e., a classification tree)\n",
    "\n",
    "\n",
    "\n",
    "- $$\\alpha^*_m(x) = argmin \\sum_i w_{m,i} exp(-\\alpha_m    y_i \\phi^*_m(x_i)) = argmin[ \\sum_{y_i = \\phi_m(x_i)} w_{m,i} e^{-\\alpha_m} + \\sum_{y_i \\neq \\phi_m(x_i)} w_{m,i} e^{\\alpha_m}] = argmin [(e^{\\alpha_m} - e^{-\\alpha_m})\\sum_i w_{m,i}  I(y_i \\neq \\phi_m(x_i)) + e^{-\\alpha_m} \\sum_i w_{m,i}] = argmin [(e^{\\alpha_m} - e^{-\\alpha_m}) err_m + e^{-\\alpha_m} \\times 1] = \\frac{1}{2}log \\frac{1-err_m}{err_m}$$\n",
    "\n",
    "---\n",
    "\n",
    "4. Logistic loss\n",
    "    \n",
    "<img src=\"https://slideplayer.com/6982498/24/images/53/Boosting+and+Logistic+Regression.jpg\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBT\n",
    "    \n",
    "- (***Regularization***) Added regularization for trees (Number of leaves + L2 norm of leave weights) for better generalization\n",
    "<img src = \"http://xijun-album.oss-cn-hangzhou.aliyuncs.com/Ensembling/p7.png\" width = \"300\">\n",
    "\n",
    "- (***Second Order***) Taylor Expansion Approximation of Loss\n",
    "    - In GBDT, we have first-order derivative (negative gradient)\n",
    "    - Generally we have $f(x + \\Delta x) = f(x) + f'(x)\\Delta x + \\frac{1}{2}f''(x) (\\Delta x)^2 + ...$\n",
    "    - In this case: ($3^{rd}$ order is useless since $\\frac{\\partial^3 L}{\\partial f} = 0$\n",
    "<img src = \"http://xijun-album.oss-cn-hangzhou.aliyuncs.com/Ensembling/p8.png\" width = \"400\">\n",
    "\n",
    "<img src = \"http://xijun-album.oss-cn-hangzhou.aliyuncs.com/Ensembling/p9.png\" width = \"500\">\n",
    "   \n",
    "- (***Bind final objective with tree building***) The goal of tree each iteration is to find a decision tree $f_t(X)$ so as to minimize objective (Gain + Complexity Cost): $$\\sum_i[g_if_t(x_i) + \\frac {1}{2} h_i f_t^2(x_i)] + \\Omega(f_t)$$\n",
    "\n",
    "- Next Step: find how to split into $J$ regions, and for each region, what is the optimal weight $w_j$.\n",
    "\n",
    "- $w^*_j$ is derived first, then node split\n",
    "    - In GBDT, squared error is minimized for node splitting\n",
    "    - In XGBoost, directly bind the split criteria to the minimization goal defined in previous step\n",
    "\n",
    "\n",
    "- Other improvements\n",
    "    - **Random subset of features** of each node just like random forest to reduce variance\n",
    "    - **Parallel feature finding** at each node to improve computational speed\n",
    "\n",
    "\n",
    "- Details: https://xgboost.readthedocs.io/en/latest/tutorials/model.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM\n",
    "- From Microsoft\n",
    "    - https://github.com/Microsoft/LightGBM/blob/master/docs/Features.rst#references\n",
    "    \n",
    "    \n",
    "- Problem: \n",
    "    - too many features\n",
    "    - too many data\n",
    "\n",
    "\n",
    "- (***Less Data***) Gradient-based One-sided Sampling (GOSS)\n",
    "    - Select top a% large gradient samples\n",
    "    - Select randomly b% low gradient samples, and scaling them up to recover the original data distribution\n",
    "\n",
    "\n",
    "- (***Less Feature***) Exclusive Feature Bundling (EFB)\n",
    "    - Bind sparse feature\n",
    "    - From (Data $\\times$ Features) to (Data $\\times$ Bundles)\n",
    "    \n",
    "    \n",
    "- (***Better Tree***) Leaf-wise grow instead of Level-wise grow \n",
    "    - The resulting tree may be towarding left side)\n",
    "    - But with same number of nodes, the $\\Delta Loss$ can be greater compared with level-wise\n",
    "    - Regularize by imposing constraints on tree depth\n",
    "\n",
    "\n",
    "- (***Categorical Split***) Splits for categorical data\n",
    "    - Instead of one-hot encoding, partition its categories into 2 subsets.\n",
    "\n",
    "\n",
    "- (***Less Computation***) Histogram \n",
    "    - Bucket continuous variables into discrete bins\n",
    "    - No need for sorting like xgboost\n",
    "    - Reduces computation complexity (from no. data to no. bins).\n",
    "    - Reduces memory usage/requirements \n",
    "    - Avoid unneccesary computation by calculating *Parent Node* and *One Child* with less data. The other child node can be calculated by *Parent* - *Child*\n",
    " \n",
    " \n",
    "- (***Better parallelizing***) reduces communication\n",
    "    - Feature Parallel: each worker has full feature set of data (instead of subset); performs selection on subset; \n",
    "    - Data Parellel: merge histogram on local features set of each worker (instead of merging global histograms from all local histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare bagging with boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bagging (RF)\n",
    "    - Focus: reduce variance\n",
    "    - Reduce variance by building independent trees and aggregating\n",
    "    - Reduce bias by using deeper trees\n",
    "    \n",
    "    \n",
    "- Boosting (GBDT)\n",
    "    - Focus: reduce bias\n",
    "    - Reduce variance by using shallow/simple trees\n",
    "    - Reduce bias by sequentially fitting error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVD and SVD "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVD\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*NhjyqD7L6N4S7Nf-FCYT-Q.png\" width=500>\n",
    "\n",
    "## SVD\n",
    "<img src=\"https://blogs.sas.com/content/iml/files/2017/08/svd1.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- http://explained.ai/gradient-boosting/index.html\n",
    "- https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1 and L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1：From Figure\n",
    "<img src=\"./fig/l1_l2.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2: Solve for minumum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difference in loss function:\n",
    "- L1: $L_1(w) = L(w) + C|w|$ \n",
    "- L2: $L_2(w) = L(w) + Cw^2$ \n",
    "\n",
    "Take L1 as example:\n",
    "- Calculate: $\\frac{\\partial L_1(w)}{\\partial w}$\n",
    "- When w<0: $f_l = \\frac{\\partial L_1(w)}{\\partial w} = L'(w) - C$\n",
    "- When w>0: $f_r = \\frac{\\partial L_1(w)}{\\partial w} = L'(w) + C$\n",
    "- If $|L'(w)|<C$ is met (i.e., C is large enough), then we have $f_l<0$ and $f_r>0$, thus minimum is find at $w=0$\n",
    "\n",
    "\n",
    "Take L2 as example:\n",
    "- $\\frac{\\partial L_2(w)}{\\partial w} = L'(w) + 2Cw$\n",
    "- Unless $L'(w=0) =0$, minimum is not at $w=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 3: Bayesian Posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the posterior for parameter:  \n",
    "$$P(\\theta|D) = \\frac{P(D|\\theta)P(\\theta)}{P(D)}$$\n",
    "\n",
    "Remove constants:\n",
    "$$P(\\theta|D) = P(D|\\theta)P(\\theta)$$\n",
    "\n",
    "Solve for $\\theta$:\n",
    "$$\\theta = argmin\\ \\{-[lnP(D|\\theta) + lnP(\\theta)]\\} =  argmin\\ [L(\\theta) - ln(P(\\theta)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For L1: $\\theta$~Laplace Disribution\n",
    "$$P(\\theta) = \\frac{1}{2b}e^{-\\frac{|\\theta|}{2b}}$$\n",
    "$$\\theta = argmin\\ [L(\\theta) + C|\\theta|]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For L2: $\\theta$~Guassian Disribution\n",
    "$$P(\\theta) = \\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{\\theta^2}{2\\sigma^2}}$$\n",
    "$$\\theta = argmin\\ [L(\\theta) + C\\theta^2]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laplace: compared with Guassian, more likely to take zero:\n",
    "<img src=\"./fig/laplace.png\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# AUC and ROC curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./fig/auc.png\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- AUC: For a random (+) and a random (-) sample, the probability that S(+) > S(-)\n",
    "- Explains why AUC equals to the area under the curve of TPR and FPR:\n",
    "\n",
    "$$AUC = \\sum P(S(+)>S(-)|+,-) \\cdot P(+,-) = \\sum_{-} P(S(+)>S(-)|-) = \\sum_{-}[TPR|Threshold = S(-)]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {},
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "threshold": 4,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "447px",
    "left": "0px",
    "right": "987px",
    "top": "134px",
    "width": "299px"
   },
   "toc_section_display": true,
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
